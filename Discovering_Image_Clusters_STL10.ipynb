{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4f1fd8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STL-10 UNSUPERVISED LEARNING - COMPLETE PIPELINE\n",
    "# No Deep Learning - Pure Traditional ML\n",
    "# ============================================================================\n",
    "\n",
    "# Install required packages\n",
    "!pip install numpy matplotlib seaborn scikit-learn scikit-image pandas umap-learn torch torchvision\n",
    "\n",
    "# Import all libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skimage.feature import hog, local_binary_pattern\n",
    "from skimage.color import rgb2gray\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: LOAD STL-10 DATASET\n",
    "# ============================================================================\n",
    "\n",
    "from torchvision.datasets import STL10\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING STL-10 DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load unlabeled dataset\n",
    "dataset = STL10(root='./data', split='unlabeled', download=True)\n",
    "\n",
    "# Sample for computational efficiency\n",
    "np.random.seed(42)\n",
    "n_samples = 15000\n",
    "sample_indices = np.random.choice(len(dataset), n_samples, replace=False)\n",
    "\n",
    "print(f\"Sampling {n_samples} images from {len(dataset)} total unlabeled images...\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "images = []\n",
    "for idx in sample_indices:\n",
    "    img, _ = dataset[idx]\n",
    "    images.append(np.array(img))\n",
    "    if len(images) % 1000 == 0:\n",
    "        print(f\"Loaded {len(images)}/{n_samples} images...\")\n",
    "\n",
    "images = np.array(images)\n",
    "print(f\"\\nâœ… Dataset loaded successfully!\")\n",
    "print(f\"Dataset shape: {images.shape}\")\n",
    "print(f\"Image dimensions: {images[0].shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: EXPLORATORY DATA ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display sample images\n",
    "fig, axes = plt.subplots(4, 5, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "random_indices = np.random.choice(len(images), 20, replace=False)\n",
    "\n",
    "for i, idx in enumerate(random_indices):\n",
    "    axes[i].imshow(images[idx])\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f'Image {idx}', fontsize=10)\n",
    "\n",
    "plt.suptitle('Sample Images from STL-10 Unlabeled Dataset', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig('01_sample_images.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Analyze color distribution\n",
    "mean_r = images[:, :, :, 0].mean(axis=(1, 2))\n",
    "mean_g = images[:, :, :, 1].mean(axis=(1, 2))\n",
    "mean_b = images[:, :, :, 2].mean(axis=(1, 2))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].hist(mean_r, bins=50, color='red', alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Red Channel Distribution', fontweight='bold')\n",
    "axes[0].set_xlabel('Mean Red Value')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1].hist(mean_g, bins=50, color='green', alpha=0.7, edgecolor='black')\n",
    "axes[1].set_title('Green Channel Distribution', fontweight='bold')\n",
    "axes[1].set_xlabel('Mean Green Value')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "axes[2].hist(mean_b, bins=50, color='blue', alpha=0.7, edgecolor='black')\n",
    "axes[2].set_title('Blue Channel Distribution', fontweight='bold')\n",
    "axes[2].set_xlabel('Mean Blue Value')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('02_color_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean Red: {mean_r.mean():.2f} Â± {mean_r.std():.2f}\")\n",
    "print(f\"Mean Green: {mean_g.mean():.2f} Â± {mean_g.std():.2f}\")\n",
    "print(f\"Mean Blue: {mean_b.mean():.2f} Â± {mean_b.std():.2f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: FEATURE EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE EXTRACTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 3.1 Color Histogram Features\n",
    "print(\"\\nExtracting color histogram features...\")\n",
    "color_features = []\n",
    "for i, img in enumerate(images):\n",
    "    hist_r = np.histogram(img[:,:,0], bins=32, range=(0,256))[0]\n",
    "    hist_g = np.histogram(img[:,:,1], bins=32, range=(0,256))[0]\n",
    "    hist_b = np.histogram(img[:,:,2], bins=32, range=(0,256))[0]\n",
    "    hist_combined = np.concatenate([hist_r, hist_g, hist_b])\n",
    "    hist_normalized = hist_combined / (hist_combined.sum() + 1e-6)\n",
    "    color_features.append(hist_normalized)\n",
    "    if (i+1) % 1000 == 0:\n",
    "        print(f\"  Processed {i+1}/{len(images)} images...\")\n",
    "\n",
    "color_features = np.array(color_features)\n",
    "print(f\"âœ… Color histogram features: {color_features.shape}\")\n",
    "\n",
    "# 3.2 HOG Features\n",
    "print(\"\\nExtracting HOG features...\")\n",
    "hog_features = []\n",
    "for i, img in enumerate(images):\n",
    "    gray = rgb2gray(img)\n",
    "    hog_feat = hog(gray, orientations=9, pixels_per_cell=(8, 8),\n",
    "                   cells_per_block=(2, 2), visualize=False, feature_vector=True)\n",
    "    hog_features.append(hog_feat)\n",
    "    if (i+1) % 1000 == 0:\n",
    "        print(f\"  Processed {i+1}/{len(images)} images...\")\n",
    "\n",
    "hog_features = np.array(hog_features)\n",
    "print(f\"âœ… HOG features: {hog_features.shape}\")\n",
    "\n",
    "# 3.3 Statistical Features\n",
    "print(\"\\nExtracting statistical features...\")\n",
    "stat_features = []\n",
    "for i, img in enumerate(images):\n",
    "    feats = []\n",
    "    for channel in range(3):\n",
    "        ch = img[:, :, channel]\n",
    "        feats.extend([ch.mean(), ch.std(), ch.min(), ch.max(), \n",
    "                     np.median(ch), np.percentile(ch, 25), np.percentile(ch, 75)])\n",
    "    feats.append(img.mean())\n",
    "    feats.append(img.std())\n",
    "    stat_features.append(feats)\n",
    "    if (i+1) % 1000 == 0:\n",
    "        print(f\"  Processed {i+1}/{len(images)} images...\")\n",
    "\n",
    "stat_features = np.array(stat_features)\n",
    "print(f\"âœ… Statistical features: {stat_features.shape}\")\n",
    "\n",
    "# 3.4 Texture Features (LBP)\n",
    "print(\"\\nExtracting texture features (LBP)...\")\n",
    "texture_features = []\n",
    "for i, img in enumerate(images):\n",
    "    gray = rgb2gray(img)\n",
    "    gray = (gray * 255).astype(np.uint8)\n",
    "    radius = 3\n",
    "    n_points = 8 * radius\n",
    "    lbp = local_binary_pattern(gray, n_points, radius, method='uniform')\n",
    "    n_bins = int(lbp.max() + 1)\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins))\n",
    "    hist = hist / (hist.sum() + 1e-6)\n",
    "    texture_features.append(hist)\n",
    "    if (i+1) % 1000 == 0:\n",
    "        print(f\"  Processed {i+1}/{len(images)} images...\")\n",
    "\n",
    "max_len = max(len(f) for f in texture_features)\n",
    "texture_features = np.array([np.pad(f, (0, max_len - len(f))) for f in texture_features])\n",
    "print(f\"âœ… Texture features: {texture_features.shape}\")\n",
    "\n",
    "# 3.5 Combine all features\n",
    "print(\"\\nCombining all features...\")\n",
    "all_features = np.hstack([color_features, hog_features, stat_features, texture_features])\n",
    "print(f\"âœ… Combined features shape: {all_features.shape}\")\n",
    "print(f\"Total feature dimensions: {all_features.shape[1]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: DIMENSIONALITY REDUCTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIMENSIONALITY REDUCTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Standardize features\n",
    "print(\"\\nStandardizing features...\")\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(all_features)\n",
    "\n",
    "# Apply PCA\n",
    "print(\"\\nApplying PCA...\")\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "features_pca = pca.fit_transform(features_scaled)\n",
    "\n",
    "print(f\"Original dimensions: {features_scaled.shape[1]}\")\n",
    "print(f\"PCA dimensions: {features_pca.shape[1]}\")\n",
    "print(f\"Explained variance: {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "\n",
    "# Visualize explained variance\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), linewidth=2)\n",
    "plt.xlabel('Number of Components', fontsize=12)\n",
    "plt.ylabel('Cumulative Explained Variance', fontsize=12)\n",
    "plt.title('PCA Explained Variance', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(range(1, min(21, len(pca.explained_variance_ratio_)+1)), \n",
    "        pca.explained_variance_ratio_[:20], alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Principal Component', fontsize=12)\n",
    "plt.ylabel('Explained Variance Ratio', fontsize=12)\n",
    "plt.title('Top 20 Principal Components', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('03_pca_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: FIND OPTIMAL NUMBER OF CLUSTERS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINDING OPTIMAL NUMBER OF CLUSTERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "k_range = range(5, 21)\n",
    "results = {'k': [], 'inertia': [], 'silhouette': [], 'davies_bouldin': [], 'calinski_harabasz': []}\n",
    "\n",
    "print(\"\\nEvaluating different values of K...\")\n",
    "for k in k_range:\n",
    "    print(f\"Testing K={k}...\", end=' ')\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10, max_iter=300)\n",
    "    labels = kmeans.fit_predict(features_pca)\n",
    "    \n",
    "    results['k'].append(k)\n",
    "    results['inertia'].append(kmeans.inertia_)\n",
    "    results['silhouette'].append(silhouette_score(features_pca, labels))\n",
    "    results['davies_bouldin'].append(davies_bouldin_score(features_pca, labels))\n",
    "    results['calinski_harabasz'].append(calinski_harabasz_score(features_pca, labels))\n",
    "    \n",
    "    print(f\"Silhouette={results['silhouette'][-1]:.3f}\")\n",
    "\n",
    "cluster_metrics = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLUSTERING EVALUATION METRICS\")\n",
    "print(\"=\"*80)\n",
    "print(cluster_metrics.to_string(index=False))\n",
    "\n",
    "# Plot all metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "axes[0, 0].plot(cluster_metrics['k'], cluster_metrics['inertia'], 'o-', linewidth=2, markersize=8, color='#2E86AB')\n",
    "axes[0, 0].set_xlabel('Number of Clusters (K)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Inertia', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title('Elbow Method', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_xticks(cluster_metrics['k'])\n",
    "\n",
    "axes[0, 1].plot(cluster_metrics['k'], cluster_metrics['silhouette'], 'o-', linewidth=2, markersize=8, color='#A23B72')\n",
    "axes[0, 1].set_xlabel('Number of Clusters (K)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Silhouette Score', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_title('Silhouette Score (Higher = Better)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_xticks(cluster_metrics['k'])\n",
    "best_sil_idx = cluster_metrics['silhouette'].idxmax()\n",
    "axes[0, 1].axvline(x=cluster_metrics.loc[best_sil_idx, 'k'], color='red', linestyle='--', alpha=0.5, label='Optimal K')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "axes[1, 0].plot(cluster_metrics['k'], cluster_metrics['davies_bouldin'], 'o-', linewidth=2, markersize=8, color='#F18F01')\n",
    "axes[1, 0].set_xlabel('Number of Clusters (K)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Davies-Bouldin Index', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_title('Davies-Bouldin Index (Lower = Better)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_xticks(cluster_metrics['k'])\n",
    "best_db_idx = cluster_metrics['davies_bouldin'].idxmin()\n",
    "axes[1, 0].axvline(x=cluster_metrics.loc[best_db_idx, 'k'], color='red', linestyle='--', alpha=0.5, label='Optimal K')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "axes[1, 1].plot(cluster_metrics['k'], cluster_metrics['calinski_harabasz'], 'o-', linewidth=2, markersize=8, color='#6A994E')\n",
    "axes[1, 1].set_xlabel('Number of Clusters (K)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Calinski-Harabasz Score', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_title('Calinski-Harabasz Score (Higher = Better)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_xticks(cluster_metrics['k'])\n",
    "best_ch_idx = cluster_metrics['calinski_harabasz'].idxmax()\n",
    "axes[1, 1].axvline(x=cluster_metrics.loc[best_ch_idx, 'k'], color='red', linestyle='--', alpha=0.5, label='Optimal K')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('04_optimal_k_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "optimal_k = int(cluster_metrics.loc[cluster_metrics['silhouette'].idxmax(), 'k'])\n",
    "print(f\"\\nðŸŽ¯ OPTIMAL K = {optimal_k} (based on highest Silhouette Score)\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: APPLY MULTIPLE CLUSTERING ALGORITHMS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"APPLYING CLUSTERING ALGORITHMS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# K-Means\n",
    "print(f\"\\n[1] K-Means Clustering (K={optimal_k})...\")\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=20, max_iter=500)\n",
    "labels_kmeans = kmeans.fit_predict(features_pca)\n",
    "silhouette_kmeans = silhouette_score(features_pca, labels_kmeans)\n",
    "print(f\"    Silhouette Score: {silhouette_kmeans:.4f}\")\n",
    "print(f\"    Cluster sizes: {np.bincount(labels_kmeans)}\")\n",
    "\n",
    "# Hierarchical\n",
    "print(f\"\\n[2] Hierarchical Clustering (K={optimal_k})...\")\n",
    "hierarchical = AgglomerativeClustering(n_clusters=optimal_k, linkage='ward')\n",
    "labels_hierarchical = hierarchical.fit_predict(features_pca)\n",
    "silhouette_hierarchical = silhouette_score(features_pca, labels_hierarchical)\n",
    "print(f\"    Silhouette Score: {silhouette_hierarchical:.4f}\")\n",
    "print(f\"    Cluster sizes: {np.bincount(labels_hierarchical)}\")\n",
    "\n",
    "# Gaussian Mixture\n",
    "print(f\"\\n[3] Gaussian Mixture Model (K={optimal_k})...\")\n",
    "gmm = GaussianMixture(n_components=optimal_k, random_state=42, n_init=10)\n",
    "labels_gmm = gmm.fit_predict(features_pca)\n",
    "silhouette_gmm = silhouette_score(features_pca, labels_gmm)\n",
    "print(f\"    Silhouette Score: {silhouette_gmm:.4f}\")\n",
    "print(f\"    Cluster sizes: {np.bincount(labels_gmm)}\")\n",
    "\n",
    "# DBSCAN\n",
    "print(f\"\\n[4] DBSCAN Clustering...\")\n",
    "dbscan = DBSCAN(eps=3.5, min_samples=10)\n",
    "labels_dbscan = dbscan.fit_predict(features_pca)\n",
    "n_clusters_dbscan = len(set(labels_dbscan)) - (1 if -1 in labels_dbscan else 0)\n",
    "n_noise = list(labels_dbscan).count(-1)\n",
    "print(f\"    Clusters found: {n_clusters_dbscan}\")\n",
    "print(f\"    Noise points: {n_noise}\")\n",
    "if n_clusters_dbscan > 1 and n_noise < len(labels_dbscan):\n",
    "    mask_valid = labels_dbscan != -1\n",
    "    silhouette_dbscan = silhouette_score(features_pca[mask_valid], labels_dbscan[mask_valid])\n",
    "    print(f\"    Silhouette Score: {silhouette_dbscan:.4f}\")\n",
    "else:\n",
    "    silhouette_dbscan = 0\n",
    "\n",
    "# Compare algorithms\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALGORITHM COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "comparison = pd.DataFrame({\n",
    "    'Algorithm': ['K-Means', 'Hierarchical', 'Gaussian Mixture', 'DBSCAN'],\n",
    "    'Silhouette Score': [silhouette_kmeans, silhouette_hierarchical, silhouette_gmm, silhouette_dbscan],\n",
    "    'N Clusters': [optimal_k, optimal_k, optimal_k, n_clusters_dbscan]\n",
    "})\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "best_algo_idx = comparison['Silhouette Score'].idxmax()\n",
    "best_algorithm = comparison.loc[best_algo_idx, 'Algorithm']\n",
    "print(f\"\\nðŸ† Best Algorithm: {best_algorithm}\")\n",
    "\n",
    "if best_algorithm == 'K-Means':\n",
    "    final_labels = labels_kmeans\n",
    "elif best_algorithm == 'Hierarchical':\n",
    "    final_labels = labels_hierarchical\n",
    "elif best_algorithm == 'Gaussian Mixture':\n",
    "    final_labels = labels_gmm\n",
    "else:\n",
    "    final_labels = labels_dbscan\n",
    "    optimal_k = n_clusters_dbscan\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: 2D VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2D VISUALIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# t-SNE\n",
    "print(\"\\nComputing t-SNE...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000, verbose=1)\n",
    "features_tsne = tsne.fit_transform(features_pca)\n",
    "\n",
    "# PCA 2D\n",
    "print(\"\\nComputing 2D PCA...\")\n",
    "pca_2d = PCA(n_components=2, random_state=42)\n",
    "features_pca_2d = pca_2d.fit_transform(features_pca)\n",
    "print(f\"2D PCA explained variance: {pca_2d.explained_variance_ratio_.sum():.4f}\")\n",
    "\n",
    "# Plot both\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "scatter1 = axes[0].scatter(features_tsne[:, 0], features_tsne[:, 1], c=final_labels, cmap='tab20', alpha=0.6, s=10, edgecolors='none')\n",
    "axes[0].set_title(f't-SNE Visualization\\n({best_algorithm} Clustering)', fontsize=16, fontweight='bold')\n",
    "axes[0].set_xlabel('t-SNE Component 1', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('t-SNE Component 2', fontsize=12, fontweight='bold')\n",
    "cbar1 = plt.colorbar(scatter1, ax=axes[0])\n",
    "cbar1.set_label('Cluster ID', fontsize=12, fontweight='bold')\n",
    "\n",
    "scatter2 = axes[1].scatter(features_pca_2d[:, 0], features_pca_2d[:, 1], c=final_labels, cmap='tab20', alpha=0.6, s=10, edgecolors='none')\n",
    "axes[1].set_title(f'PCA 2D Visualization\\n({best_algorithm} Clustering)', fontsize=16, fontweight='bold')\n",
    "axes[1].set_xlabel('First Principal Component', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Second Principal Component', fontsize=12, fontweight='bold')\n",
    "cbar2 = plt.colorbar(scatter2, ax=axes[1])\n",
    "cbar2.set_label('Cluster ID', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('05_2d_visualization.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 8: VISUALIZE CLUSTER SAMPLES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLUSTER SAMPLE VISUALIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "samples_per_cluster = 10\n",
    "fig, axes = plt.subplots(optimal_k, samples_per_cluster, figsize=(20, 2.5*optimal_k))\n",
    "\n",
    "if optimal_k == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for cluster_id in range(optimal_k):\n",
    "    cluster_mask = final_labels == cluster_id\n",
    "    cluster_indices = np.where(cluster_mask)[0]\n",
    "    \n",
    "    if len(cluster_indices) == 0:\n",
    "        continue\n",
    "    \n",
    "    n_samples_in_cluster = len(cluster_indices)\n",
    "    \n",
    "    if n_samples_in_cluster >= samples_per_cluster:\n",
    "        sample_idx = np.random.choice(cluster_indices, samples_per_cluster, replace=False)\n",
    "    else:\n",
    "        sample_idx = cluster_indices\n",
    "    \n",
    "    for i in range(samples_per_cluster):\n",
    "        if i < len(sample_idx):\n",
    "            axes[cluster_id, i].imshow(images[sample_idx[i]])\n",
    "            axes[cluster_id, i].axis('off')\n",
    "            if i == 0:\n",
    "                axes[cluster_id, i].set_title(f'Cluster {cluster_id}\\n({n_samples_in_cluster} images)', \n",
    "                                             fontsize=11, fontweight='bold', loc='left')\n",
    "        else:\n",
    "            axes[cluster_id, i].axis('off')\n",
    "\n",
    "plt.suptitle(f'Sample Images from Each Cluster ({best_algorithm})', fontsize=18, fontweight='bold', y=0.998)\n",
    "plt.tight_layout()\n",
    "plt.savefig('06_cluster_samples.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 9: CLUSTER CHARACTERISTICS ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLUSTER CHARACTERISTICS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "characteristics = []\n",
    "for cluster_id in range(optimal_k):\n",
    "    cluster_mask = final_labels == cluster_id\n",
    "    cluster_images = images[cluster_mask]\n",
    "    \n",
    "    if len(cluster_images) == 0:\n",
    "        continue\n",
    "    \n",
    "    avg_brightness = cluster_images.mean()\n",
    "    std_brightness = cluster_images.std()\n",
    "    avg_red = cluster_images[:, :, :, 0].mean()\n",
    "    avg_green = cluster_images[:, :, :, 1].mean()\n",
    "    avg_blue = cluster_images[:, :, :, 2].mean()\n",
    "    color_variance = cluster_images.std(axis=(0, 1, 2)).mean()\n",
    "    \n",
    "    characteristics.append({\n",
    "        'Cluster': cluster_id,\n",
    "        'Size': cluster_mask.sum(),\n",
    "        'Avg_Brightness': avg_brightness,\n",
    "        'Std_Brightness': std_brightness,\n",
    "        'Avg_Red': avg_red,\n",
    "        'Avg_Green': avg_green,\n",
    "        'Avg_Blue': avg_blue,\n",
    "        'Color_Variance': color_variance\n",
    "    })\n",
    "\n",
    "cluster_stats = pd.DataFrame(characteristics)\n",
    "print(\"\\nCluster Statistics:\")\n",
    "print(cluster_stats.to_string(index=False))\n",
    "\n",
    "# Visualize cluster characteristics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "axes[0, 0].bar(cluster_stats['Cluster'], cluster_stats['Size'], color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Cluster ID', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Number of Images', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title('Cluster Sizes', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "axes[0, 1].bar(cluster_stats['Cluster'], cluster_stats['Avg_Brightness'], color='orange', edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Cluster ID', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Average Brightness', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_title('Average Brightness per Cluster', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "x = np.arange(len(cluster_stats))\n",
    "width = 0.25\n",
    "axes[1, 0].bar(x - width, cluster_stats['Avg_Red'], width, label='Red', color='red', alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].bar(x, cluster_stats['Avg_Green'], width, label='Green', color='green', alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].bar(x + width, cluster_stats['Avg_Blue'], width, label='Blue', color='blue', alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Cluster ID', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Average Color Value', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_title('Average RGB Values per Cluster', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(cluster_stats['Cluster'])\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "axes[1, 1].bar(cluster_stats['Cluster'], cluster_stats['Color_Variance'], color='purple', edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Cluster ID', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Color Variance', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_title('Color Variance per Cluster', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('07_cluster_characteristics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 10: CLUSTER INTERPRETATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLUSTER INTERPRETATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for _, row in cluster_stats.iterrows():\n",
    "    cluster_id = int(row['Cluster'])\n",
    "    brightness = row['Avg_Brightness']\n",
    "    red = row['Avg_Red']\n",
    "    green = row['Avg_Green']\n",
    "    blue = row['Avg_Blue']\n",
    "    \n",
    "    rgb = [red, green, blue]\n",
    "    colors = ['Red', 'Green', 'Blue']\n",
    "    dominant_color = colors[np.argmax(rgb)]\n",
    "    \n",
    "    if brightness < 85:\n",
    "        brightness_cat = \"Dark\"\n",
    "    elif brightness < 170:\n",
    "        brightness_cat = \"Medium\"\n",
    "    else:\n",
    "        brightness_cat = \"Bright\"\n",
    "    \n",
    "    interpretation = f\"{brightness_cat} images with {dominant_color}-ish tones\"\n",
    "    \n",
    "    print(f\"\\nCluster {cluster_id} ({int(row['Size'])} images):\")\n",
    "    print(f\"  - {interpretation}\")\n",
    "    print(f\"  - Brightness: {brightness:.1f}/255\")\n",
    "    print(f\"  - RGB: R={red:.1f}, G={green:.1f}, B={blue:.1f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 11: FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL PROJECT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = f\"\"\"\n",
    "PROJECT: Discovering Image Clusters in STL-10 Dataset\n",
    "METHOD: Unsupervised Machine Learning\n",
    "\n",
    "DATASET INFORMATION:\n",
    "  â€¢ Total images analyzed: {len(images):,}\n",
    "  â€¢ Image dimensions: {images.shape[1]}Ã—{images.shape[2]}Ã—{images.shape[3]}\n",
    "\n",
    "FEATURE EXTRACTION:\n",
    "  â€¢ Color Histograms (RGB, 32 bins)\n",
    "  â€¢ HOG (Histogram of Oriented Gradients)\n",
    "  â€¢ Statistical Features\n",
    "  â€¢ Texture Features (LBP)\n",
    "  â€¢ Total features: {all_features.shape[1]:,} dimensions\n",
    "\n",
    "DIMENSIONALITY REDUCTION:\n",
    "  â€¢ Method: PCA\n",
    "  â€¢ Reduced to: {features_pca.shape[1]} components\n",
    "  â€¢ Variance explained: {pca.explained_variance_ratio_.sum():.2%}\n",
    "\n",
    "CLUSTERING:\n",
    "  â€¢ Best Algorithm: {best_algorithm}\n",
    "  â€¢ Optimal clusters: {optimal_k}\n",
    "  â€¢ Silhouette Score: {comparison.loc[best_algo_idx, 'Silhouette Score']:.4f}\n",
    "\n",
    "VISUALIZATION:\n",
    "  â€¢ 2D projections: t-SNE and PCA\n",
    "  â€¢ Cluster sample grids\n",
    "  â€¢ Statistical analysis charts\n",
    "\n",
    "âœ… PROJECT COMPLETE!\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "with open('project_summary.txt', 'w') as f:\n",
    "    f.write(summary)\n",
    "    f.write(\"\\n\\nCLUSTER STATISTICS:\\n\")\n",
    "    f.write(cluster_stats.to_string(index=False))\n",
    "\n",
    "print(\"\\nâœ… Summary saved to 'project_summary.txt'\")\n",
    "print(\"âœ… All visualizations saved\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
